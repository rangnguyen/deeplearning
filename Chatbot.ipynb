{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rangnguyen/deeplearning/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui08ZvTy9gZ_",
        "colab_type": "code",
        "outputId": "eb6eeec6-fbb8-4f23-fe65-2060b62d8f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "import os\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTMF5-4L9mNW",
        "colab_type": "code",
        "outputId": "11407bbe-5855-446b-ae7e-3aca29f2754e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLZr-uWoL7Rn",
        "colab_type": "code",
        "outputId": "2766e41a-b928-4f0c-afe8-40bea2eb3acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "path = \"/content/gdrive/My Drive/Colab Notebooks/Chap09/cornell-moviedialog-corpus/\"\n",
        "print(os.listdir(path))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.DS_Store', 'movie_titles_metadata.txt', 'movie_conversations.txt', 'README.txt', 'movie_lines.txt', 'raw_script_urls.txt', 'chameleons.pdf', 'movie_characters_metadata.txt', 'model_attention.h5']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42ahf3x3Lozg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNekf2v2T3cX",
        "colab_type": "text"
      },
      "source": [
        "# **Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NI2CQAgT3-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lines = open(path + 'movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open(path + 'movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX1a0NRbUUH4",
        "colab_type": "text"
      },
      "source": [
        "# **Create a dictionary to map each line's id with its text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cseCADpEUWha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THXEtg7kUb5f",
        "colab_type": "text"
      },
      "source": [
        "# **Create a list of all of the conversations' lines' ids.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9qU6RV8UeHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convs = []\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XDPyaA0Uj5u",
        "colab_type": "text"
      },
      "source": [
        "#**id and conversation sample**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl3bQR6fUkMD",
        "colab_type": "code",
        "outputId": "d0c0f3b5-e881-4a2b-c7e9-c0e757369161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "for k in convs[3]:\n",
        "    print (k, id2line[k])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L204 Why?\n",
            "L205 Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
            "L206 That's a shame.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEQ5XNfRUqnf",
        "colab_type": "text"
      },
      "source": [
        "#Sort the sentences into questions (inputs) and answers (targets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwxUZ0qpUuuv",
        "colab_type": "code",
        "outputId": "d9b71b29-2669-4f57-902e-945e37072105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        questions.append(id2line[conv[i]])\n",
        "        answers.append(id2line[conv[i+1]])\n",
        "        \n",
        "# Compare lengths of questions and answers\n",
        "print(len(questions))\n",
        "print(len(answers))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "221616\n",
            "221616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5toO14euU6Nl",
        "colab_type": "text"
      },
      "source": [
        "# Clean text by removing unnecessary characters and altering the format of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzmiX3OoU6aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv8IcXN7VCUD",
        "colab_type": "text"
      },
      "source": [
        "# Clean the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eSCc0KOVCfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "clean_answers = []    \n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KecqDi8VLB9",
        "colab_type": "text"
      },
      "source": [
        "# Find the length of sentences (not using nltk due to processing speed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FruqjWwBVL95",
        "colab_type": "code",
        "outputId": "df82d34e-f3df-4b5d-bbe8-23ea7cfe5c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "lengths = []\n",
        "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
        "for question in clean_questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.0\n",
            "19.0\n",
            "24.0\n",
            "32.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dncRROcVTV3",
        "colab_type": "text"
      },
      "source": [
        "# Remove questions and answers that are shorter than 1 word and longer than 20 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLfWPWA2VTnm",
        "colab_type": "code",
        "outputId": "b218bee4-b808-4f33-e8a0-dc8b4837eb7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(clean_questions):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "        \n",
        "print(len(short_questions))\n",
        "print(len(short_answers))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138528\n",
            "138528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624-2C7cVcZL",
        "colab_type": "text"
      },
      "source": [
        "# Randomly check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQDmtRiLVch_",
        "colab_type": "code",
        "outputId": "af50307d-843b-471a-e606-1de6ac4626ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "r = np.random.randint(1,len(short_questions))\n",
        "\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "then why did your king believe in you without any proof?\n",
            "go and ask him yourself.\n",
            "\n",
            "his face.\n",
            "does he have hair?\n",
            "\n",
            "is it long and hanging down?\n",
            "i am more interested in what he says, not what he looks like.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txhViaGwVqb4",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing for word based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "086Io1GlVsMK",
        "colab_type": "code",
        "outputId": "0e8a8bab-a8d0-4030-e3c6-98e157c03d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "num_samples = 100000  # Number of samples to train on.\n",
        "short_questions = short_questions[:num_samples]\n",
        "short_answers = short_answers[:num_samples]\n",
        "#tokenizing the qns and answers\n",
        "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
        "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mdkc_OUV9Zr",
        "colab_type": "text"
      },
      "source": [
        "#train-validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppuE5uqJV-YL",
        "colab_type": "code",
        "outputId": "0ca1b4d6-b9c5-4bf1-e02e-3e66d4cd615a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data_size = len(short_questions_tok)\n",
        "\n",
        "# We will use the first 0-80th %-tile (80%) of data for the training\n",
        "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
        "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
        "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
        "\n",
        "# We will use the remaining for validation\n",
        "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
        "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
        "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
        "\n",
        "training_size = len(training_input)\n",
        "validation_size = len(validation_input)\n",
        "print('training size', training_size)\n",
        "print('validation size', validation_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training size 80000\n",
            "validation size 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Poz1RRB_WFwO",
        "colab_type": "text"
      },
      "source": [
        "# Word en/decoding dictionaries\n",
        "Create a dictionary for the frequency of the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQILi27SWWew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = {}\n",
        "for question in short_questions_tok:\n",
        "    for word in question:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "\n",
        "for answer in short_answers_tok:\n",
        "    for word in answer:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eMLxApRWbdn",
        "colab_type": "text"
      },
      "source": [
        "# Remove rare words from the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIIC6fKBWe7i",
        "colab_type": "code",
        "outputId": "5bd51691-a292-4358-efbb-5ef7e394e908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "threshold = 5\n",
        "count = 0\n",
        "for k,v in vocab.items():\n",
        "    if v >= threshold:\n",
        "        count += 1\n",
        "print(\"Size of total vocab:\", len(vocab))\n",
        "print(\"Size of vocab we will use:\", count)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 34148\n",
            "Size of vocab we will use: 10835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdXHOALVWrIR",
        "colab_type": "text"
      },
      "source": [
        "#we will create dictionaries to provide a unique integer for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOM0Osk4Wqht",
        "colab_type": "code",
        "outputId": "40da51dd-540f-4d45-a59d-d9d6f644fdae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "WORD_CODE_START = 1\n",
        "WORD_CODE_PADDING = 0\n",
        "\n",
        "\n",
        "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
        "encoding = {}\n",
        "decoding = {1: '<STA>'}\n",
        "for word, count in vocab.items():\n",
        "    if count >= threshold: #get vocabularies that appear above threshold count\n",
        "        encoding[word] = word_num \n",
        "        decoding[word_num ] = word\n",
        "        word_num += 1\n",
        "\n",
        "print(\"No. of vocab used:\", word_num)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of vocab used: 10837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3aYkkjDW23c",
        "colab_type": "code",
        "outputId": "b231f192-0095-4a99-b1b8-cd667f7d16eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#include unknown token for words not in dictionary\n",
        "decoding[word_num] = '<UNK>'\n",
        "encoding['<UNK>'] = word_num\n",
        "dict_size = word_num+1\n",
        "dict_size"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10838"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhDuN9lgXAOp",
        "colab_type": "text"
      },
      "source": [
        "# Vectorizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulcPrhl6W-Ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(encoding, data, vector_size=20):\n",
        "    \"\"\"\n",
        "    :param encoding: encoding dict built by build_word_encoding()\n",
        "    :param data: list of strings\n",
        "    :param vector_size: size of each encoded vector\n",
        "    \"\"\"\n",
        "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
        "    for i in range(len(data)):\n",
        "        for j in range(min(len(data[i]), vector_size)):\n",
        "            try:\n",
        "                transformed_data[i][j] = encoding[data[i][j]]\n",
        "            except:\n",
        "                transformed_data[i][j] = encoding['<UNK>']\n",
        "    return transformed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZMkqnOMXOq7",
        "colab_type": "code",
        "outputId": "d86f59b5-09da-418d-b996-545ea8255e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#encoding training set\n",
        "encoded_training_input = transform(\n",
        "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
        "encoded_training_output = transform(\n",
        "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_training_input', encoded_training_input.shape)\n",
        "print('encoded_training_output', encoded_training_output.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_training_input (80000, 20)\n",
            "encoded_training_output (80000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6_71QT7XR0n",
        "colab_type": "code",
        "outputId": "89e7ef77-0fda-4754-b77b-6c7eb3c8ef7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#encoding validation set\n",
        "encoded_validation_input = transform(\n",
        "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
        "encoded_validation_output = transform(\n",
        "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_validation_input', encoded_validation_input.shape)\n",
        "print('encoded_validation_output', encoded_validation_output.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_validation_input (20000, 20)\n",
            "encoded_validation_output (20000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzrh-6LXXWdh",
        "colab_type": "text"
      },
      "source": [
        "# 2 Model Building\n",
        "2.1 Sequence-to-Sequence in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_X8meUHXctV",
        "colab_type": "code",
        "outputId": "409b101f-29e9-45f7-9f89-f8235bdc5fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20\n",
        "\n",
        "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
        "decoder_input = Input(shape=(OUTPUT_LENGTH,))\n",
        "\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
        "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
        "encoder_last = encoder[:,-1,:]\n",
        "\n",
        "print('encoder', encoder)\n",
        "print('encoder_last', encoder_last)\n",
        "\n",
        "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
        "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
        "\n",
        "print('decoder', decoder)\n",
        "\n",
        "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
        "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0808 23:08:36.074041 140360782862208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0808 23:08:36.075205 140360782862208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0808 23:08:36.078861 140360782862208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0808 23:08:36.721026 140360782862208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2888: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "encoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
            "encoder_last Tensor(\"strided_slice:0\", shape=(?, 512), dtype=float32)\n",
            "decoder Tensor(\"lstm_2/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY6lS-FCXp5Q",
        "colab_type": "text"
      },
      "source": [
        "2.2 Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ea9G-aLXqk2",
        "colab_type": "code",
        "outputId": "45f5b888-9451-4a0c-99bc-6a6208d6291e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from keras.layers import Activation, dot, concatenate\n",
        "\n",
        "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
        "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
        "attention = dot([decoder, encoder], axes=[2, 2])\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "print('attention', attention)\n",
        "\n",
        "context = dot([attention, encoder], axes=[2,1])\n",
        "print('context', context)\n",
        "\n",
        "decoder_combined_context = concatenate([context, decoder])\n",
        "print('decoder_combined_context', decoder_combined_context)\n",
        "\n",
        "# Has another weight + tanh layer as described in equation (5) of the paper\n",
        "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
        "print('output', output)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention Tensor(\"attention/truediv:0\", shape=(?, 20, 20), dtype=float32)\n",
            "context Tensor(\"dot_2/MatMul:0\", shape=(?, 20, 512), dtype=float32)\n",
            "decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 20, 1024), dtype=float32)\n",
            "output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 20, 10838), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAxqFq62XvXi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569rgrqXXyNj",
        "colab_type": "code",
        "outputId": "79badb2b-0ff5-4152-89be-43a18f809c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0808 23:08:39.418643 140360782862208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0808 23:08:39.443480 140360782862208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 20, 128)      1387264     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 128)      1387264     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 20, 10838)    5559894     time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 11,484,758\n",
            "Trainable params: 11,484,758\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTKCcux4X4yR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfMY61sYX492",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_encoder_input = encoded_training_input\n",
        "training_decoder_input = np.zeros_like(encoded_training_output)\n",
        "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
        "training_decoder_input[:, 0] = WORD_CODE_START\n",
        "training_decoder_output = encoded_training_output.reshape(training_size, INPUT_LENGTH, 1)\n",
        "#training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
        "\n",
        "validation_encoder_input = encoded_validation_input\n",
        "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
        "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
        "validation_decoder_input[:, 0] = WORD_CODE_START\n",
        "validation_decoder_output = encoded_validation_output.reshape(validation_size, OUTPUT_LENGTH, 1)\n",
        "#validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCcOgwU2YoSB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmm9zT9JYoa-",
        "colab_type": "code",
        "outputId": "0eaf003f-5f97-439c-bf97-ba5a96f03623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],    \n",
        "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
        "          #validation_split=0.05,\n",
        "          batch_size=128, epochs=10)\n",
        "\n",
        "model.save(path + 'model_attention.h5')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "80000/80000 [==============================] - 64s 805us/step - loss: 3.1349 - val_loss: 4.5045\n",
            "Epoch 2/10\n",
            "80000/80000 [==============================] - 64s 806us/step - loss: 3.0697 - val_loss: 4.5256\n",
            "Epoch 3/10\n",
            "80000/80000 [==============================] - 64s 802us/step - loss: 3.0040 - val_loss: 4.5584\n",
            "Epoch 4/10\n",
            "80000/80000 [==============================] - 64s 802us/step - loss: 2.9392 - val_loss: 4.5952\n",
            "Epoch 5/10\n",
            "80000/80000 [==============================] - 64s 800us/step - loss: 2.8741 - val_loss: 4.6395\n",
            "Epoch 6/10\n",
            "80000/80000 [==============================] - 64s 799us/step - loss: 2.8097 - val_loss: 4.6736\n",
            "Epoch 7/10\n",
            "80000/80000 [==============================] - 64s 800us/step - loss: 2.7462 - val_loss: 4.7064\n",
            "Epoch 8/10\n",
            "80000/80000 [==============================] - 64s 799us/step - loss: 2.6855 - val_loss: 4.7461\n",
            "Epoch 9/10\n",
            "80000/80000 [==============================] - 64s 799us/step - loss: 2.6237 - val_loss: 4.7786\n",
            "Epoch 10/10\n",
            "80000/80000 [==============================] - 64s 798us/step - loss: 2.5648 - val_loss: 4.8169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'strided_slice:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'strided_slice:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks07KcG-1Ku_",
        "colab_type": "text"
      },
      "source": [
        "# 3. Model testing¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGXPSv_0-mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(raw_input):\n",
        "    clean_input = clean_text(raw_input)\n",
        "    input_tok = [nltk.word_tokenize(clean_input)]\n",
        "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
        "    encoder_input = transform(encoding, input_tok, 20)\n",
        "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
        "    decoder_input[:,0] = WORD_CODE_START\n",
        "    for i in range(1, OUTPUT_LENGTH):\n",
        "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
        "        decoder_input[:,i] = output[:,i]\n",
        "    return output\n",
        "\n",
        "def decode(decoding, vector):\n",
        "    \"\"\"\n",
        "    :param decoding: decoding dict built by word encoding\n",
        "    :param vector: an encoded vector\n",
        "    \"\"\"\n",
        "    text = ''\n",
        "    for i in vector:\n",
        "        if i == 0:\n",
        "            break\n",
        "        text += ' '\n",
        "        text += decoding[i]\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4XOJXBnspQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rXKR0oe1JlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a1befa47-f56b-4181-bba9-0bedb6f2276c"
      },
      "source": [
        "    short_questions = 'Hello!'\n",
        "    output = prediction(short_questions)\n",
        "    print ('Q:', short_questions)\n",
        "    print ('A:', decode(decoding, output[0]))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: Hello!\n",
            "A:  hello , i am going to the party , smokey ?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}