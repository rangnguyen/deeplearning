{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"license_plate_ocr.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"5DRJJtjaHvVs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":721},"outputId":"0d0074f3-1105-4988-89c2-add7d0245efc","executionInfo":{"status":"ok","timestamp":1565920311945,"user_tz":-420,"elapsed":24708,"user":{"displayName":"Rang Nguyen","photoUrl":"https://lh3.googleusercontent.com/-vQAcnF2bz2E/AAAAAAAAAAI/AAAAAAAABP8/fsqiF_DYqO8/s64/photo.jpg","userId":"05008177146404682146"}}},"source":["!pip install -q cairocffi editdistance\n","!apt install -q libcairo2-dev"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |████▉                           | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.5MB/s \n","\u001b[?25h  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-410\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  libcairo-script-interpreter2 libpixman-1-dev libxcb-shm0-dev\n","Suggested packages:\n","  libcairo2-doc\n","The following NEW packages will be installed:\n","  libcairo-script-interpreter2 libcairo2-dev libpixman-1-dev libxcb-shm0-dev\n","0 upgraded, 4 newly installed, 0 to remove and 4 not upgraded.\n","Need to get 930 kB of archives.\n","After this operation, 3,986 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo-script-interpreter2 amd64 1.15.10-2ubuntu0.1 [53.5 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-dev amd64 0.34.0-2 [244 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-shm0-dev amd64 1.13-2~ubuntu18.04 [6,684 B]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo2-dev amd64 1.15.10-2ubuntu0.1 [626 kB]\n","Fetched 930 kB in 1s (1,011 kB/s)\n","Selecting previously unselected package libcairo-script-interpreter2:amd64.\n","(Reading database ... 131289 files and directories currently installed.)\n","Preparing to unpack .../libcairo-script-interpreter2_1.15.10-2ubuntu0.1_amd64.deb ...\n","Unpacking libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\n","Selecting previously unselected package libpixman-1-dev:amd64.\n","Preparing to unpack .../libpixman-1-dev_0.34.0-2_amd64.deb ...\n","Unpacking libpixman-1-dev:amd64 (0.34.0-2) ...\n","Selecting previously unselected package libxcb-shm0-dev:amd64.\n","Preparing to unpack .../libxcb-shm0-dev_1.13-2~ubuntu18.04_amd64.deb ...\n","Unpacking libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\n","Selecting previously unselected package libcairo2-dev:amd64.\n","Preparing to unpack .../libcairo2-dev_1.15.10-2ubuntu0.1_amd64.deb ...\n","Unpacking libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\n","Setting up libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\n","Setting up libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Setting up libpixman-1-dev:amd64 (0.34.0-2) ...\n","Setting up libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ql3R3yWIHn7U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"22999e85-c3e6-463e-98f2-9484fed046b0","executionInfo":{"status":"ok","timestamp":1565920414453,"user_tz":-420,"elapsed":992,"user":{"displayName":"Rang Nguyen","photoUrl":"https://lh3.googleusercontent.com/-vQAcnF2bz2E/AAAAAAAAAAI/AAAAAAAABP8/fsqiF_DYqO8/s64/photo.jpg","userId":"05008177146404682146"}}},"source":["import keras\n","import tensorflow as tf\n","print('TensorFlow version:', tf.__version__)\n","print('Keras version:', keras.__version__)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["TensorFlow version: 1.14.0\n","Keras version: 2.2.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8KvjMkC8HwsG","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"5sljxm3WHn7h","colab_type":"code","colab":{}},"source":["import os\n","from os.path import join\n","import json\n","import random\n","import itertools\n","import re\n","import datetime\n","import cairocffi as cairo\n","import editdistance\n","import numpy as np\n","from scipy import ndimage\n","import pylab\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from keras import backend as K\n","from keras.layers.convolutional import Conv2D, MaxPooling2D\n","from keras.layers import Input, Dense, Activation\n","from keras.layers import Reshape, Lambda\n","from keras.layers.merge import add, concatenate\n","from keras.models import Model, load_model\n","from keras.layers.recurrent import GRU\n","from keras.optimizers import SGD\n","from keras.utils.data_utils import get_file\n","from keras.preprocessing import image\n","import keras.callbacks\n","import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uizrNLavHn7n","colab_type":"code","colab":{}},"source":["sess = tf.Session()\n","K.set_session(sess)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkEdk6k4Hn7s","colab_type":"text"},"source":["# Get alphabet"]},{"cell_type":"code","metadata":{"id":"8UCcDCBeHn7u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":361},"outputId":"6af2b36b-cae2-488e-ba0f-a82e63421178","executionInfo":{"status":"error","timestamp":1565920426232,"user_tz":-420,"elapsed":852,"user":{"displayName":"Rang Nguyen","photoUrl":"https://lh3.googleusercontent.com/-vQAcnF2bz2E/AAAAAAAAAAI/AAAAAAAABP8/fsqiF_DYqO8/s64/photo.jpg","userId":"05008177146404682146"}}},"source":["from collections import Counter\n","def get_counter(dirpath, tag):\n","    dirname = os.path.basename(dirpath)\n","    ann_dirpath = join(dirpath, 'ann')\n","    letters = ''\n","    lens = []\n","    for filename in os.listdir(ann_dirpath):\n","        json_filepath = join(ann_dirpath, filename)\n","        ann = json.load(open(json_filepath, 'r'))\n","        tags = ann['tags']\n","        if tag in tags:\n","            description = ann['description']\n","            lens.append(len(description))\n","            letters += description\n","    print('Max plate length in \"%s\":' % dirname, max(Counter(lens).keys()))\n","    return Counter(letters)\n","c_val = get_counter('/data/anpr_ocr__train', 'val')\n","c_train = get_counter('/data/anpr_ocr__train', 'train')\n","letters_train = set(c_train.keys())\n","letters_val = set(c_val.keys())\n","if letters_train == letters_val:\n","    print('Letters in train and val do match')\n","else:\n","    raise Exception()\n","# print(len(letters_train), len(letters_val), len(letters_val | letters_train))\n","letters = sorted(list(letters_train))\n","print('Letters:', ' '.join(letters))"],"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-a80aafc5c20d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max plate length in \"%s\":'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/anpr_ocr__train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/anpr_ocr__train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mletters_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-a80aafc5c20d>\u001b[0m in \u001b[0;36mget_counter\u001b[0;34m(dirpath, tag)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mletters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_dirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mjson_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_dirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/anpr_ocr__train/ann'"]}]},{"cell_type":"markdown","metadata":{"id":"M2TkPxviHn71","colab_type":"text"},"source":["# Input data generator"]},{"cell_type":"code","metadata":{"id":"QO_VLl7HHn74","colab_type":"code","colab":{}},"source":["def labels_to_text(labels):\n","    return ''.join(list(map(lambda x: letters[int(x)], labels)))\n","\n","def text_to_labels(text):\n","    return list(map(lambda x: letters.index(x), text))\n","\n","def is_valid_str(s):\n","    for ch in s:\n","        if not ch in letters:\n","            return False\n","    return True\n","\n","class TextImageGenerator:\n","    \n","    def __init__(self, \n","                 dirpath,\n","                 tag,\n","                 img_w, img_h, \n","                 batch_size, \n","                 downsample_factor,\n","                 max_text_len=8):\n","        \n","        self.img_h = img_h\n","        self.img_w = img_w\n","        self.batch_size = batch_size\n","        self.max_text_len = max_text_len\n","        self.downsample_factor = downsample_factor\n","        \n","        img_dirpath = join(dirpath, 'img')\n","        ann_dirpath = join(dirpath, 'ann')\n","        self.samples = []\n","        for filename in os.listdir(img_dirpath):\n","            name, ext = os.path.splitext(filename)\n","            if ext in ['.png', '.jpg']:\n","                img_filepath = join(img_dirpath, filename)\n","                json_filepath = join(ann_dirpath, name + '.json')\n","                ann = json.load(open(json_filepath, 'r'))\n","                description = ann['description']\n","                tags = ann['tags']\n","                if tag not in tags:\n","                    continue\n","                if is_valid_str(description):\n","                    self.samples.append([img_filepath, description])\n","        \n","        self.n = len(self.samples)\n","        self.indexes = list(range(self.n))\n","        self.cur_index = 0\n","        \n","    def build_data(self):\n","        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n","        self.texts = []\n","        for i, (img_filepath, text) in enumerate(self.samples):\n","            img = cv2.imread(img_filepath)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","            img = cv2.resize(img, (self.img_w, self.img_h))\n","            img = img.astype(np.float32)\n","            img /= 255\n","            # width and height are backwards from typical Keras convention\n","            # because width is the time dimension when it gets fed into the RNN\n","            self.imgs[i, :, :] = img\n","            self.texts.append(text)\n","        \n","    def get_output_size(self):\n","        return len(letters) + 1\n","    \n","    def next_sample(self):\n","        self.cur_index += 1\n","        if self.cur_index >= self.n:\n","            self.cur_index = 0\n","            random.shuffle(self.indexes)\n","        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n","    \n","    def next_batch(self):\n","        while True:\n","            # width and height are backwards from typical Keras convention\n","            # because width is the time dimension when it gets fed into the RNN\n","            if K.image_data_format() == 'channels_first':\n","                X_data = np.ones([self.batch_size, 1, self.img_w, self.img_h])\n","            else:\n","                X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])\n","            Y_data = np.ones([self.batch_size, self.max_text_len])\n","            input_length = np.ones((self.batch_size, 1)) * (self.img_w // self.downsample_factor - 2)\n","            label_length = np.zeros((self.batch_size, 1))\n","            source_str = []\n","                                   \n","            for i in range(self.batch_size):\n","                img, text = self.next_sample()\n","                img = img.T\n","                if K.image_data_format() == 'channels_first':\n","                    img = np.expand_dims(img, 0)\n","                else:\n","                    img = np.expand_dims(img, -1)\n","                X_data[i] = img\n","                Y_data[i] = text_to_labels(text)\n","                source_str.append(text)\n","                label_length[i] = len(text)\n","                \n","            inputs = {\n","                'the_input': X_data,\n","                'the_labels': Y_data,\n","                'input_length': input_length,\n","                'label_length': label_length,\n","                #'source_str': source_str\n","            }\n","            outputs = {'ctc': np.zeros([self.batch_size])}\n","            yield (inputs, outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSBk93kWHn79","colab_type":"code","colab":{}},"source":["tiger = TextImageGenerator('/data/anpr_ocr__train', 'val', 128, 64, 8, 4)\n","tiger.build_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZlkP69wHn8E","colab_type":"code","colab":{}},"source":["for inp, out in tiger.next_batch():\n","    print('Text generator output (data which will be fed into the neutral network):')\n","    print('1) the_input (image)')\n","    if K.image_data_format() == 'channels_first':\n","        img = inp['the_input'][0, 0, :, :]\n","    else:\n","        img = inp['the_input'][0, :, :, 0]\n","    \n","    plt.imshow(img.T, cmap='gray')\n","    plt.show()\n","    print('2) the_labels (plate number): %s is encoded as %s' % \n","          (labels_to_text(inp['the_labels'][0]), list(map(int, inp['the_labels'][0]))))\n","    print('3) input_length (width of image that is fed to the loss function): %d == %d / 4 - 2' % \n","          (inp['input_length'][0], tiger.img_w))\n","    print('4) label_length (length of plate number): %d' % inp['label_length'][0])\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8l1HlJJHn8M","colab_type":"text"},"source":["# Loss and train functions, network architecture"]},{"cell_type":"code","metadata":{"id":"vC7COKgkHn8N","colab_type":"code","colab":{}},"source":["def ctc_lambda_func(args):\n","    y_pred, labels, input_length, label_length = args\n","    # the 2 is critical here since the first couple outputs of the RNN\n","    # tend to be garbage:\n","    y_pred = y_pred[:, 2:, :]\n","    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n","\n","\n","def train(img_w, load=False):\n","    # Input Parameters\n","    img_h = 64\n","\n","    # Network parameters\n","    conv_filters = 16\n","    kernel_size = (3, 3)\n","    pool_size = 2\n","    time_dense_size = 32\n","    rnn_size = 512\n","\n","    if K.image_data_format() == 'channels_first':\n","        input_shape = (1, img_w, img_h)\n","    else:\n","        input_shape = (img_w, img_h, 1)\n","        \n","    batch_size = 32\n","    downsample_factor = pool_size ** 2\n","    tiger_train = TextImageGenerator('/data/anpr_ocr__train', 'train', img_w, img_h, batch_size, downsample_factor)\n","    tiger_train.build_data()\n","    tiger_val = TextImageGenerator('/data/anpr_ocr__train', 'val', img_w, img_h, batch_size, downsample_factor)\n","    tiger_val.build_data()\n","\n","    act = 'relu'\n","    input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n","    inner = Conv2D(conv_filters, kernel_size, padding='same',\n","                   activation=act, kernel_initializer='he_normal',\n","                   name='conv1')(input_data)\n","    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n","    inner = Conv2D(conv_filters, kernel_size, padding='same',\n","                   activation=act, kernel_initializer='he_normal',\n","                   name='conv2')(inner)\n","    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n","\n","    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\n","    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n","\n","    # cuts down input size going into RNN:\n","    inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n","\n","    # Two layers of bidirecitonal GRUs\n","    # GRU seems to work as well, if not better than LSTM:\n","    gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner)\n","    gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner)\n","    gru1_merged = add([gru_1, gru_1b])\n","    gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged)\n","    gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n","\n","    # transforms RNN output to character activations:\n","    inner = Dense(tiger_train.get_output_size(), kernel_initializer='he_normal',\n","                  name='dense2')(concatenate([gru_2, gru_2b]))\n","    y_pred = Activation('softmax', name='softmax')(inner)\n","    Model(inputs=input_data, outputs=y_pred).summary()\n","\n","    labels = Input(name='the_labels', shape=[tiger_train.max_text_len], dtype='float32')\n","    input_length = Input(name='input_length', shape=[1], dtype='int64')\n","    label_length = Input(name='label_length', shape=[1], dtype='int64')\n","    # Keras doesn't currently support loss funcs with extra parameters\n","    # so CTC loss is implemented in a lambda layer\n","    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n","\n","    # clipnorm seems to speeds up convergence\n","    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n","\n","    if load:\n","        model = load_model('./tmp_model.h5', compile=False)\n","    else:\n","        model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n","\n","    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n","    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n","    \n","    if not load:\n","        # captures output of softmax so we can decode the output during visualization\n","        test_func = K.function([input_data], [y_pred])\n","\n","        model.fit_generator(generator=tiger_train.next_batch(), \n","                            steps_per_epoch=tiger_train.n,\n","                            epochs=1, \n","                            validation_data=tiger_val.next_batch(), \n","                            validation_steps=tiger_val.n)\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nDcssBoCHn8S","colab_type":"text"},"source":["# Model description and training"]},{"cell_type":"markdown","metadata":{"id":"FE3dapuEHn8T","colab_type":"text"},"source":["Next block will take about 30 minutes."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"nzYmqeRNHn8U","colab_type":"code","colab":{}},"source":["model = train(128, load=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dF9HLX4dHn8Z","colab_type":"text"},"source":["# Function to decode neural network output"]},{"cell_type":"code","metadata":{"id":"IPpXhJ8kHn8a","colab_type":"code","colab":{}},"source":["# For a real OCR application, this should be beam search with a dictionary\n","# and language model.  For this example, best path is sufficient.\n","\n","def decode_batch(out):\n","    ret = []\n","    for j in range(out.shape[0]):\n","        out_best = list(np.argmax(out[j, 2:], 1))\n","        out_best = [k for k, g in itertools.groupby(out_best)]\n","        outstr = ''\n","        for c in out_best:\n","            if c < len(letters):\n","                outstr += letters[c]\n","        ret.append(outstr)\n","    return ret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nEuQ2qtHn8e","colab_type":"text"},"source":["# Test on validation images"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"DZIDH61iHn8f","colab_type":"code","colab":{}},"source":["tiger_test = TextImageGenerator('/data/anpr_ocr__test', 'test', 128, 64, 8, 4)\n","tiger_test.build_data()\n","\n","net_inp = model.get_layer(name='the_input').input\n","net_out = model.get_layer(name='softmax').output\n","\n","for inp_value, _ in tiger_test.next_batch():\n","    bs = inp_value['the_input'].shape[0]\n","    X_data = inp_value['the_input']\n","    net_out_value = sess.run(net_out, feed_dict={net_inp:X_data})\n","    pred_texts = decode_batch(net_out_value)\n","    labels = inp_value['the_labels']\n","    texts = []\n","    for label in labels:\n","        text = ''.join(list(map(lambda x: letters[int(x)], label)))\n","        texts.append(text)\n","    \n","    for i in range(bs):\n","        fig = plt.figure(figsize=(10, 10))\n","        outer = gridspec.GridSpec(2, 1, wspace=10, hspace=0.1)\n","        ax1 = plt.Subplot(fig, outer[0])\n","        fig.add_subplot(ax1)\n","        ax2 = plt.Subplot(fig, outer[1])\n","        fig.add_subplot(ax2)\n","        print('Predicted: %s\\nTrue: %s' % (pred_texts[i], texts[i]))\n","        img = X_data[i][:, :, 0].T\n","        ax1.set_title('Input img')\n","        ax1.imshow(img, cmap='gray')\n","        ax1.set_xticks([])\n","        ax1.set_yticks([])\n","        ax2.set_title('Activations')\n","        ax2.imshow(net_out_value[i].T, cmap='binary', interpolation='nearest')\n","        ax2.set_yticks(list(range(len(letters) + 1)))\n","        ax2.set_yticklabels(letters + ['blank'])\n","        ax2.grid(False)\n","        for h in np.arange(-0.5, len(letters) + 1 + 0.5, 1):\n","            ax2.axhline(h, linestyle='-', color='k', alpha=0.5, linewidth=1)\n","        \n","        #ax.axvline(x, linestyle='--', color='k')\n","        plt.show()\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTSr22GDHn8j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}